{"meta":{"title":"Esse Quam Videri","subtitle":null,"description":null,"author":"y0ngh00n","url":"https://y0ngh00n.github.io"},"pages":[],"posts":[{"title":"Ch6_Deep Feedforward Networks","slug":"dlb-ch6","date":"2018-04-01T11:33:46.000Z","updated":"2018-04-01T12:32:28.005Z","comments":true,"path":"2018/04/01/dlb-ch6/","link":"","permalink":"https://y0ngh00n.github.io/2018/04/01/dlb-ch6/","excerpt":"","text":"Feedforward No feedback connections 피드백이 있다면 RNN이다. linear model의 한계를 극복하려고 생겨났다. 선형 모델의 한계는 model capacity가 linear function에 국한되어있고, two input변수에 안맞는다는 것. XOR Linear model(단층 퍼셉트론)로는 XOR함수를 풀 수 없다. Gradient-Based Learning 뉴럴넷을 학습시키는 것은 기존의 머신 러닝 모델에서 사용하는 gradient descent와 같다. 가장 큰 다른점은 뉴럴넷의 nonlinearity가 손실함수의 non convex를 야기한다는 것이다? Ans) 기존 머신 러닝 optimization 방법은 linear programming등 여러 가지가 있다. 이는 모두 convex가 보장된다는 전제가 있다. (여기서, convex는 concave(위로 볼록)을 구분하지 않는다.) 하지만 nonlinearity는 non convex를 의미한다. 예를 들어, 여러 지점의 볼록성을 가질 수 있는데 이때를 saddle point라고 한다. 이러한 saddle point는 기존 optimization으로 해결 되지 않는다. Cost functionsMaximum likelihood estimationx와 파라미터가 주어졌을 때 y가 최대로 하는 확률일 때의 파라미터머신러닝에서는 posterior를 구하는 것이 중요하다.예를 들어 사과와 오렌지의 확률이 9:1로 주어질 때 prior가 사과라면 posterior도 사과일 확률은?일반적으로 prior가 uniform하다고 가정한다. The negative loglikelihood == the cross entropya. 기울기가 너무 작아지는 것을 방지한다.b. 가우시안 분포를 가정할 때 크로스 엔트로피의 최소화는 평균제곱오차(Mean Squared Error)의 최소화와 본질적으로 동일함. Conditional Statisticsa. MSEb. MAE Output unitsOutput을 어떻게 표현할 지에 따라서 cross-entropy 함수의 형태가 달라진다. Linear units가우시안 분포일때 사용 Sigmoid units베르누이 분포 = 이항분포일때 사용 (binary-classification) Softmax units멀티누이 분포 = 이산변수일때 사용 (multi-classification) Others Hidden units 이론적으로 뚜렷한 가이드라인이 아직 없다. activation function: threshold보다 클 때에만 뉴런 활성화 ReLUsigmoid의 vanish gradient 문제를 해결 왜?max(0,z): 0이하면 0, 이상이면 그대로 출력 generalization 방법 Absolute value ReLUa. 0이하일 때 기울기를 -1로 고정하여 출력 값을 |z|b. 이미지 Object recognition Leaky ReLUa. 0이하일 때 기울기를 0.01 등 작게 고정함 PReLU(parametric)a. 0이하일 때 기울기를 as a learnable parameter Maxout unitsa. 두 개의 W와 b 중에서 큰 값이 나온 것을 사용하는 함수? Ans) ReLU는 0과 output을 비교하지만, maxout은 이전것에서 올라온 것과 output을 비교해서 큰 것을 사용한다..? 좀 더 공부 필요b. ReLU와 같이 일반적으로 쓰이며 성능이 좋다. Logistic sigmoid and hyperbolic tangentReLU 이전에 쓰던 활성화 함수 Logistic sigmoida. 0~1 범위, 확률값b. gradient saturation의 문제 Tanha. sigmoid의 개량형, -1~1 범위 RNN, 확률모델, 오토인코더, gate에 쓰임 others Architecture design Universial approximation properties and depth Universal approximation theorema. 1개의 hidden layer를 가진 뉴럴넷으로 어떤 함수든 근사시킬 수 있다.b. Deep 모델을 shallow로 변환하면 parameter 수가 exponential하게 증가한다. Deep model을 하는 이유a. 같은 parameter 개수라도 path가 복잡해진다.b. 하나의 커다란 유닛을 가진 single layer보다 deep한 layer 모델이 유닛수를 적게하면서 더 나은 함수를 근사하고, generalization error 또한 줄일 수 있다. 왜? Ans) Generalization: new 데이터가 들어왔 때 설명력이 얼마나 되는가의 수치 Local representation in shallow nna. 한 반을 기준으로 비슷한 아이를 찾아낸다고 할 때, 새로 온 학생은 영희와 비슷하다는 식. 즉, 다른 데이터 샘플과 유사성을 찾는다. Distributive representation in deep nna. 공통적인 속성 눈,코,입 등은 제하고 feature의 특징을 찾는다. Distributive representation이 feature selection을 더 잘 하기 때문에 새로운 데이터가 들어왔을 때 generalization error가 작다. Deep layer의 특징반복되는, 규칙적인 패턴을 찾을 수 있다. Other considerations Skip connectionsa. i th layer -&gt; i+2 th layerb. 일반적으로 layer가 체인으로 연결될 필요는 없다. 오히려, skip connections을 통해 Gradient가 output layer로 잘 흐르게 한다. 왜? Ans) backprop을 통해 gradient를 계산할때 곱셈에서 0인 부분이 있으면 계속해서 0이 전달되므로 정보 손실을 본다. 때문에 skip connection을 통해 gradient의 0을 줄이므로 optimization이 잘 된다. Fewer connectionsa. Layer 간에 연결을 fully connected가 아닌 일부분만 연결하는 전략.b. 이를 통해 Connection 개수, parameter 개수, 계산량 감소한다.c. 예를 들어 CNN의 sparse connections Back-propagation and other differentiation algorithms Backpropagationi. Gradient를 계산한다.ii. 단순하고 비용이 적게 든다. Computational graphsi. 계산과정을 그래프로 나타낸 것 Chain rule of calculusi. Recursively applying the chain rule to obtain backpropii. Backprop computation in fully-connected MLPiii. Symbol-to-symbol derivativesiv. General backprop Historical notes","categories":[{"name":"Study","slug":"Study","permalink":"https://y0ngh00n.github.io/categories/Study/"},{"name":"DeepLearning","slug":"Study/DeepLearning","permalink":"https://y0ngh00n.github.io/categories/Study/DeepLearning/"}],"tags":[{"name":"DLB","slug":"DLB","permalink":"https://y0ngh00n.github.io/tags/DLB/"},{"name":"feedforward","slug":"feedforward","permalink":"https://y0ngh00n.github.io/tags/feedforward/"}]},{"title":"Ubuntu 원격 서버 연동하기","slug":"dl-server","date":"2018-03-26T13:45:20.000Z","updated":"2018-03-26T15:05:26.459Z","comments":true,"path":"2018/03/26/dl-server/","link":"","permalink":"https://y0ngh00n.github.io/2018/03/26/dl-server/","excerpt":"","text":"ssh를 이용한 GPU 원격 서버 연동Multi-hop SSH서버 이용을 위해서는 외부서버 접속 -&gt; 내부서버 접속이 필요한 상황이다. 첫번째 터미널 $ ssh -l username -L 6000:inner_ip:22 outer_ip cat - //6000:포트 번호 Pycharm remote interpreter 연동첫번째 터미널을 유지한 상태에서… 프로젝트 remote interpreter 정의참고: https://stackoverflow.com/questions/34359415/pycharm-ssh-interpter-no-such-file-or-directory Here server C is the target machine and server B the gate machine. And then in the remote interpreter configuration. (File&gt;Setting&gt;Projet&gt;project interpreter&gt;click on small wheel button&gt;add remote): host: localhostport: 6000user: user_serverC (for me same as user_serverB) &lt;주의&gt; path mappings를 설정해준다.local: 내 프로젝트 디렉토리remote: 원격 서버에 배포할 디렉토리 deployment(배포) 설정root 디렉토리가 mappings의 위치가 같도록 설정해준다. deploy &amp; run coderemote에 local의 파일이 있어야 run이 된다. jupyter notebook 연동참고: https://glauffer.github.io/2017-04-01-remote-host-a-jupyter-notebook-via-multihop-ssh/첫번째 터미널을 유지한 상태에서… 두번째 터미널… $ ssh -l username -L 8888:localhost:8888 -p 6000 localhost //8888:jupyter port, localhost:host이름 $ ### password login ### $ jupyter notebook --no-browser --port=8888 TODO: pycharm에서 jupyter notebook의 실행이 안되는 것","categories":[{"name":"Study","slug":"Study","permalink":"https://y0ngh00n.github.io/categories/Study/"},{"name":"DeepLearning","slug":"Study/DeepLearning","permalink":"https://y0ngh00n.github.io/categories/Study/DeepLearning/"}],"tags":[{"name":"server","slug":"server","permalink":"https://y0ngh00n.github.io/tags/server/"},{"name":"DL","slug":"DL","permalink":"https://y0ngh00n.github.io/tags/DL/"}]},{"title":"linux 하드웨어 정보 명령어","slug":"linux","date":"2018-03-25T07:01:40.000Z","updated":"2018-03-26T14:32:40.811Z","comments":true,"path":"2018/03/25/linux/","link":"","permalink":"https://y0ngh00n.github.io/2018/03/25/linux/","excerpt":"","text":"리눅스 하드웨어 정보 확인CPU 정보 확인$ cat /proc/cpuinfo | grep \"model name\" | head -1 메모리 정보 확인$ cat /proc/meminfo 디스크 용량 확인$ df -h PCI 정보 확인$ lspci /* 모든 PCI 정보 출력 */ $ lspci | grep RAID /* RAID 모델명 확인 */ $ lspci | grep Ethernet /* 랜카드 모델명 확인 */ $ lspci | grep VGA /* 그래픽 모델명 확인 */","categories":[{"name":"Study","slug":"Study","permalink":"https://y0ngh00n.github.io/categories/Study/"},{"name":"OS","slug":"Study/OS","permalink":"https://y0ngh00n.github.io/categories/Study/OS/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://y0ngh00n.github.io/tags/linux/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-03-22T03:00:46.839Z","updated":"2018-03-26T13:39:30.225Z","comments":true,"path":"2018/03/22/hello-world/","link":"","permalink":"https://y0ngh00n.github.io/2018/03/22/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"Blog","slug":"Blog","permalink":"https://y0ngh00n.github.io/categories/Blog/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://y0ngh00n.github.io/tags/Hexo/"}]}]}