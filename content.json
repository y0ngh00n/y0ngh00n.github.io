{"meta":{"title":"今我異昨我","subtitle":null,"description":null,"author":"y0ngh00n","url":"https://y0ngh00n.github.io"},"pages":[],"posts":[{"title":"NVIDIA CUDA installation Guide for Linux","slug":"gpu-server-setting","date":"2018-08-20T00:52:35.000Z","updated":"2018-08-20T01:35:18.694Z","comments":true,"path":"2018/08/20/gpu-server-setting/","link":"","permalink":"https://y0ngh00n.github.io/2018/08/20/gpu-server-setting/","excerpt":"","text":"1. Introduction System Requirements x86_641234567891011// OS$ cat /etc/*-release// Kernel$ uname -r// GCC$ gcc -v// GLIBC$ getconf –a | grep glibc 2. Pre-installation Actions (gpuadmin) Nvidia driver 1$ .run --no-x-check Cuda 9.1 1$ .run Cudnn Set PATH in .bashrc 3. DL Framework (user) PyTorch a. Create conda env 12$ conda create –n envname anaconda pip –offline$ source activate envname b. Install .whl (dependent packages) pip install c. Add conda env to jupyter 1python -m ipykernel install --user --name envname --display-name \"Python (envname)\" 4. Referenceshttps://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.htmlhttps://dreamgonfly.github.io/2017/12/17/deeplearning-server-setting.htmlhttps://www.born2data.com/2017/deeplearning_install-part1.htmlhttps://subicura.com/2017/01/19/docker-guide-for-beginners-2.html","categories":[{"name":"Study","slug":"Study","permalink":"https://y0ngh00n.github.io/categories/Study/"},{"name":"DeepLearning","slug":"Study/DeepLearning","permalink":"https://y0ngh00n.github.io/categories/Study/DeepLearning/"}],"tags":[{"name":"server","slug":"server","permalink":"https://y0ngh00n.github.io/tags/server/"},{"name":"DL","slug":"DL","permalink":"https://y0ngh00n.github.io/tags/DL/"}]},{"title":"Ch9_Convolutional Networks","slug":"dlb-ch9","date":"2018-04-24T07:30:09.000Z","updated":"2018-04-24T07:59:53.494Z","comments":true,"path":"2018/04/24/dlb-ch9/","link":"","permalink":"https://y0ngh00n.github.io/2018/04/24/dlb-ch9/","excerpt":"","text":"intro 뉴럴넷에서 말하는 convolution operation은 전통적으로 공학과 수학에서 쓰이는 convolution 연산과 조금은 다르다. 이 장에서는 CNN이 제공하는 툴들에 대해 설명하고, 상황에 따른 적합한 툴을 선택하는 것은 11장에서 하도록 한다. 1. The Convolution Operation Kernel = filter Output = feature map 2. Motivation CNN의 세 가지 idea들a. Sparse interactions(sparse connectivity, sparse weights) ① Kernel size를 input size보다 작게 만들면서 convolution 네트워크를 구성한다. ② 큰 픽셀의 Input에서 의미있는 feature들을 작은 픽셀단위로 찾을 수 있다. ③ 효율성i. Parameter의 감소 -&gt; 메모리 감소 ii. Operation의 감소 ④ 모델의 layer가 deep해지면 directly가 아니더라도 indirectly하게 거의 모든 input unit과 연결된 효과가 있다. b. Parameter sharing(tied weight) ① 한 input layer에서의 weight를 공유한다. ② Sparse connectivity와 parameter sharing은 edge detecting에 효과적이다.c. Equivalent representations ① Equivalent: input이 변하면 output도 같이 변하는 것, f(g(x)) = g(f(x)) ② Input image에 대해 convolution하고 transform한 것과, transform먼저 하고 convolution한 결과가 같다. 3. Pooling 전형적인 CNN은 3단계를 거친다.a. Convolution stage ① Produce a set of linear functionsb. Detector stage ① Non-linear Activation functionc. Pooling stage ① Pooling functions Pooling functionsa. 특정 위치의 output을 주변 결과들의 통계요약으로 대체하는 것b. 방법 ① Max pooling ② Average pooling ③ L2 norm of a rectangular neighborhood ④ Weighted average on the distance from the central pixel 4. Convolution and Pooling as an Infinitely Strong Prior Prior probability distributiona. What models are resonable, before we have seen any data. Priora. Weak: high entropy, high varianceb. Strong: low entropy, low variances 5. Variants of the Basic Convolution Function NN의 convolution은 수학의 convolution 연산과는 다르다.a. Convolution in parallel ① 하나의 kernel은 하나의 feature만 추출할 수 있기 때문에 많은 convolution을 통해 많은 feature를 추출해야한다.b. Grid of vector-valuedd observations ① 컬러 이미지는 RGB와 같은 3D tensor이다. Convolution with a stridea. convolution연산을 kernel내 몇몇 유닛을 건너띄어 계산량을 줄이는 것.b. 1이상의 stride는 unit stride + down sampling과 같은 결과를 갖는다. Zero paddinga. Input size를 사방으로 wider하게 하는 것b. 큰 size의 Kernel을 거치고 나면 output size가 줄어드는데 층이 deep해질 경우 사이즈가 1로 shrink할 수 있다.c. 3가지 case ① Valid convolutioni. Output shrinkage ② Same convolution i. Input size와 동일한 크기의 output을 보존한다 ③ Full convolutioni. Feature가 center에만 집중된다. ④ Valid와 same 사이 그 어딘가가 적절하다. Locally connected layer(unshared convolution)a. 모든 연결이 자기자신의 weight를 가지고 있다.b. No parameter sharingc. 그렇지만 FC와는 다르다. FC는 모든 모든 edge마다 weight를 가지고 있다.sd. Useful when we know that each feature should be a function of a small part of space, but there is no reason to think that the same feature should occur across all of space..?e. Also useful partial connectivity between channels. ① 각 output channel은 input channel의 제한된 일부분만 연결되어 있다. f. Tiled convolution ① Paramter sharing에 cycle이 있는 것.(fig 9.16) 6. Structured Outputs Classification과 regression이 아닌 high dimensional, structured object의 결과도 만들 수 있다. 초기 guess값이 다음 이웃 픽셀에 영향을 준다. 7. Data Types Single channela. 1D: audio waveformb. 2D: audio with fourier transformc. 3D: volumetric data Multi channela. 1D: Skeleton animation datab. 2D: color image data (RGB)c. 3D: color video data (time, width, height) 8. Efficient Convolution Algorithms Kernal이 separable하다?a. D dim kernal can be expressed as the outer product of d vectors, one vector per dim w: kernal, d:dim Naive convolution = O(w^d) Separable convolution = O(wd) 9. Random or Unsupervised Features CNN에서 가장 expensive한 부분은 feature를 학습하는 과정이다. 그래서 CNN의 training cost를 줄이는 방법중의 하나는 gd로 학습되지 않은 feature를 사용하는 것이다. Supervised learning을 통하지 않고 feature를 얻는 방법 3가지a. Random initializationb. Design by handc. Unsupervised criterion 10. The Neuroscientific Basis for Convolutional Networks11. Convolutional Networks and the History of Deep Learning","categories":[{"name":"Study","slug":"Study","permalink":"https://y0ngh00n.github.io/categories/Study/"},{"name":"DeepLearning","slug":"Study/DeepLearning","permalink":"https://y0ngh00n.github.io/categories/Study/DeepLearning/"}],"tags":[{"name":"DLB","slug":"DLB","permalink":"https://y0ngh00n.github.io/tags/DLB/"},{"name":"CNN","slug":"CNN","permalink":"https://y0ngh00n.github.io/tags/CNN/"}]},{"title":"Ch7_Regularization","slug":"dlb-ch7","date":"2018-04-13T07:46:45.000Z","updated":"2018-04-13T08:19:30.477Z","comments":true,"path":"2018/04/13/dlb-ch7/","link":"","permalink":"https://y0ngh00n.github.io/2018/04/13/dlb-ch7/","excerpt":"","text":"용어 정리 Generalization: 일반화, 테스트 데이터의 정확도. Normalization: 정규화, 속성의 단위들을 일정 구간으로 통일 시키는 것. 최대최소정규화-&gt; 딥러닝은 normalization이 중요하다. Drastic한 성능 증가를 볼 수 있다. Standardization: 표준화, 표준정규화 Regularization(제약): generalization error(test error)를 줄이기 위한 어떠한 수정전략들. 즉, overfitting을 방지하기 위한 제약들. intro “공짜점심은 없다 이론”을 통해 학습 알고리즘 뿐만 아니라 regularization 또한 특정 데이터에 따라 방법이 다르게 적용된다. 딥러닝에서의 대부분의 regularization 전략은 estimators를 regularizing 하는 것이다. estimator를 regularizing한다는 것은 bias가 증가하는데 비해 variance가 줄어드는 상충관계가 있다. Bias와 variance는 왜 상충관계가 있는가?cf)https://www.youtube.com/watch?v=FOu8bXV15F8&amp;feature=youtu.be&amp;list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq 트레이닝 할 때 Error = bias + variance 이다.Variance: 예측값이 퍼져있는 정도Bias: 예측값과 실제값의 차이 정도Variance가 크다 =&gt; overfitting이 크다. (training accuracy와 test accuracy의 차이가 큰 것)prediction이 일정하다. =&gt; variance가 작다.Bias가 크다 =&gt; underfitting (training accuracy가 낮은 것) 1. Parameter Norm Penalties Objective function에 norm penalty를 더하면서 모델의 설명력을 제한하는 방법이다. W*: unregularization의 최적점 W~: regularization의 접점 머신러닝은 일반적으로 posterior를 구하는 것인데 posterior = likelihood * prior(gaussian dist 가정) Regularizer는 이러한 prior를 고려하는 것이다. = L2 norm ∂ : regularization strength L2 regularizationa. Weight가 큰 변수일수록 그 영향을 낮춘다. L1 regularizationa. L1 regularization의 효과 ① Sparse represenation ② L1은 lasso regression으로 절대값 함수에 의해 마름모꼴 즉, 꼭지점과 직선이 있는 도형이다. ③ 이러한 상황에서 weight의 dimension이 커질 때, 꼭지점에 접하는 경우, 즉 weight가 0인 경우가 많아질 수 있다. ④ 이런 것을 sparse representation이라고 한다.b. L2 와의 차이 ① One consequence of this form of the gradient is that we will not necessarily see clean algebraic solutions to quadratic approximations of J as we did for L2 regularization. 2. Norm Penalties as Constrained Optimization3. Regularization and Under-Constrained Problems4. Dataset Augmentation 데이터가 어떻게 생겼는지에 따라 방법이 다르다.a. Ex) 이미지가 사자와 숫자가 있다면 숫자는 회전을 하는 방법은 적절하지 않다. Generalization을 높이는 가장 좋은 방법은 train data를 늘리는 것이다. Training data로부터 Fake data를 늘리는 data augmentation은 특정한 분류 문제에서만 효과적이다. 바로, object recognition 분야. Transformation 방법은 image translation, rotating, scaling등이 있다.a. Translation: 이미지 이동. 왼쪽 위, 오른쪽 위 등ㅇ Speech recognition에서도 data augmentation은 효과적이다. 뉴럴넷 모델 Input으로 noise를 주입하는 것도 방법이다. 5. Noise Robustness 일반적으로 모델의 hidden unit에 노이즈를 주입하면 shrinking paramter보다 강력해진다. Shrinking paramters? Injecting Noise at the Output Targetsa. 많은 데이터 셋들은 일정 부분 잘못된 y값들이 존재한다.b. 이럴 경우에 y(label)에 노이즈를 명시함으로 maximize logp(y|x)를 달성하도록 도운다.c. 노이즈, 1- 노이즈d. Label smoothing ① 예를 들어 Softmax 함수에서 k개의 label이 존재할때, 노이즈/k-1과 1-노이즈로 6. Semi-Supervised Learning Unlabeled 데이터와 labeled데이터가 섞인 데이터셋에서 학습하는 것 7. Multi-Task Learning Pooling the examples arising out of several tasks. Main idea: among the factors that explain the variations observed in the data associated with the different tasks, some are shared across two or more tasks. 8. Early Stopping Overfitting을 방지하기 위해 validation set error가 증가하는 시점에 학습을 끊는 것. 모든 머신러닝 기법에 적용할 수 있는 일반적인 기법으로 혼자 쓰이거나 다른 기법과 혼용 가능하다. 학습을 미리 끊음으로 학습 과정의 비용을 줄일 수 있다. L2 penalty인 weight decay와 비슷한 결과를 갖는다 9. Parameter Tying and Parameter Sharing Parameter tying = 다른 파라미터들을 같도록 Parameter sharing = force set of parameters to be equala. 같은 output을 내는 연관 모델들의 파라미터는 비슷할 것이다.b. 장점으로는 메모리에 올리는 파라미터 셋 공간을 줄일 수 있다. CNN에서의 parameter sharinga. 한 레이어 에서 파라미터들은 같이 나눠서 쓰도록한다. 10. Sparse Representations L1 penalty의 sparse parameterization과 구분하여야한다.a. Parameter의 대부분이 sparse한 것 Input x에 대해 sparse하게 representation 하는 것이다. 11. Bagging and Other Ensemble Methods Bagging = bootstrap aggregatinga. 모델을 여러 개 결합하는 것b. 각 모델의 test set 결과에 대하여 majority votingc. Model averaging의 한 방법으로 각각 다른 모델은 각각 다른 에러를 보여주므로써 generalization 성능을 높인다. 12. Dropout Base network로부터 output unit이 아닌 곳을 삭제한 각기 다른 ensemble 모델을 구현한다. K개의 다른 데이터 셋에서 나온 k개의 모델 효과를 얻는 것이 drop out의 목표이다. 다른 regularization 방법보다 비용도 안크고 성능도 매우 좋다.a. Very computationally cheapb. 모델이나 학습 절차에 영향을 받지 않는다. Ans) coadaptation을 회피한다.a. GD를 하면서 gradient값이 같으면 그 노드는 같은 정보를 제공하는 상황이 발생하기 때문에 이를 드롭아웃을 통해 노드의 전달을 끊음으로 coadaptation을 회피한다. Test할 때에는 1-dropout probability를 곱해서 scaling한다.a. 다른 스킬 : train할 때 dropout probability만큼 곱해서 학습 13. Adversarial Training NN의 문제: 적대적인 노이즈가 끼면 아예 다른 이미지로 판단.a. 이미지에 대한 지식: 이미지는 큰 디멘션을 갖지만 그 중 정보를 담고 있는 픽셀들은 sparse하다.b. 즉, 이미지를 구성하는 feature는 sparse하게 정해져있다.c. 랜덤으로 픽셀을 찍는다고 해서 이미지가 되는 것이 아니다. X를 크게 바꾸지 않으면서, y가 크게 바뀌는 상황을 학습 Noise를 제거하도록 학습 14. Tangent Distance, Tangent Prop, and Manifold Tangent Classifiers Manifold Tangent: manifold의 접선","categories":[{"name":"Study","slug":"Study","permalink":"https://y0ngh00n.github.io/categories/Study/"},{"name":"DeepLearning","slug":"Study/DeepLearning","permalink":"https://y0ngh00n.github.io/categories/Study/DeepLearning/"}],"tags":[{"name":"DLB","slug":"DLB","permalink":"https://y0ngh00n.github.io/tags/DLB/"},{"name":"regularization","slug":"regularization","permalink":"https://y0ngh00n.github.io/tags/regularization/"}]},{"title":"Ch6_Deep Feedforward Networks","slug":"dlb-ch6","date":"2018-04-01T11:33:46.000Z","updated":"2018-04-13T08:19:32.904Z","comments":true,"path":"2018/04/01/dlb-ch6/","link":"","permalink":"https://y0ngh00n.github.io/2018/04/01/dlb-ch6/","excerpt":"","text":"Feedforward No feedback connections 피드백이 있다면 RNN이다. linear model의 한계를 극복하려고 생겨났다. 선형 모델의 한계는 model capacity가 linear function에 국한되어있고, two input변수에 안맞는다는 것. 1. XOR Linear model(단층 퍼셉트론)로는 XOR함수를 풀 수 없다. 2. Gradient-Based Learning 뉴럴넷을 학습시키는 것은 기존의 머신 러닝 모델에서 사용하는 gradient descent와 같다. 가장 큰 다른점은 뉴럴넷의 nonlinearity가 손실함수의 non convex를 야기한다는 것이다? Ans) 기존 머신 러닝 optimization 방법은 linear programming등 여러 가지가 있다. 이는 모두 convex가 보장된다는 전제가 있다. (여기서, convex는 concave(위로 볼록)을 구분하지 않는다.) 하지만 nonlinearity는 non convex를 의미한다. 예를 들어, 여러 지점의 볼록성을 가질 수 있는데 이때를 saddle point라고 한다. 이러한 saddle point는 기존 optimization으로 해결 되지 않는다. Cost functionsMaximum likelihood estimationx와 파라미터가 주어졌을 때 y가 최대로 하는 확률일 때의 파라미터머신러닝에서는 posterior를 구하는 것이 중요하다.예를 들어 사과와 오렌지의 확률이 9:1로 주어질 때 prior가 사과라면 posterior도 사과일 확률은?일반적으로 prior가 uniform하다고 가정한다. The negative loglikelihood == the cross entropya. 기울기가 너무 작아지는 것을 방지한다.b. 가우시안 분포를 가정할 때 크로스 엔트로피의 최소화는 평균제곱오차(Mean Squared Error)의 최소화와 본질적으로 동일함. Conditional Statisticsa. MSEb. MAE Output unitsOutput을 어떻게 표현할 지에 따라서 cross-entropy 함수의 형태가 달라진다. Linear units가우시안 분포일때 사용 Sigmoid units베르누이 분포 = 이항분포일때 사용 (binary-classification) Softmax units멀티누이 분포 = 이산변수일때 사용 (multi-classification) Others 3. Hidden units 이론적으로 뚜렷한 가이드라인이 아직 없다. activation function: threshold보다 클 때에만 뉴런 활성화 ReLUsigmoid의 vanish gradient 문제를 해결 왜?max(0,z): 0이하면 0, 이상이면 그대로 출력 generalization 방법 Absolute value ReLUa. 0이하일 때 기울기를 -1로 고정하여 출력 값을 |z|b. 이미지 Object recognition Leaky ReLUa. 0이하일 때 기울기를 0.01 등 작게 고정함 PReLU(parametric)a. 0이하일 때 기울기를 as a learnable parameter Maxout unitsa. 두 개의 W와 b 중에서 큰 값이 나온 것을 사용하는 함수? Ans) ReLU는 0과 output을 비교하지만, maxout은 이전것에서 올라온 것과 output을 비교해서 큰 것을 사용한다..? 좀 더 공부 필요b. ReLU와 같이 일반적으로 쓰이며 성능이 좋다. Logistic sigmoid and hyperbolic tangentReLU 이전에 쓰던 활성화 함수 Logistic sigmoida. 0~1 범위, 확률값b. gradient saturation의 문제 Tanha. sigmoid의 개량형, -1~1 범위 RNN, 확률모델, 오토인코더, gate에 쓰임 others 4. Architecture design Universial approximation properties and depth Universal approximation theorema. 1개의 hidden layer를 가진 뉴럴넷으로 어떤 함수든 근사시킬 수 있다.b. Deep 모델을 shallow로 변환하면 parameter 수가 exponential하게 증가한다. Deep model을 하는 이유a. 같은 parameter 개수라도 path가 복잡해진다.b. 하나의 커다란 유닛을 가진 single layer보다 deep한 layer 모델이 유닛수를 적게하면서 더 나은 함수를 근사하고, generalization error 또한 줄일 수 있다. 왜? Ans) Generalization: new 데이터가 들어왔 때 설명력이 얼마나 되는가의 수치 Local representation in shallow nna. 한 반을 기준으로 비슷한 아이를 찾아낸다고 할 때, 새로 온 학생은 영희와 비슷하다는 식. 즉, 다른 데이터 샘플과 유사성을 찾는다. Distributive representation in deep nna. 공통적인 속성 눈,코,입 등은 제하고 feature의 특징을 찾는다. Distributive representation이 feature selection을 더 잘 하기 때문에 새로운 데이터가 들어왔을 때 generalization error가 작다. Deep layer의 특징반복되는, 규칙적인 패턴을 찾을 수 있다. Other considerations Skip connectionsa. i th layer -&gt; i+2 th layerb. 일반적으로 layer가 체인으로 연결될 필요는 없다. 오히려, skip connections을 통해 Gradient가 output layer로 잘 흐르게 한다. 왜? Ans) backprop을 통해 gradient를 계산할때 곱셈에서 0인 부분이 있으면 계속해서 0이 전달되므로 정보 손실을 본다. 때문에 skip connection을 통해 gradient의 0을 줄이므로 optimization이 잘 된다. Fewer connectionsa. Layer 간에 연결을 fully connected가 아닌 일부분만 연결하는 전략.b. 이를 통해 Connection 개수, parameter 개수, 계산량 감소한다.c. 예를 들어 CNN의 sparse connections 5. Back-propagation and other differentiation algorithms Backpropagationi. Gradient를 계산한다.ii. 단순하고 비용이 적게 든다. Computational graphsi. 계산과정을 그래프로 나타낸 것 Chain rule of calculusi. Recursively applying the chain rule to obtain backpropii. Backprop computation in fully-connected MLPiii. Symbol-to-symbol derivativesiv. General backprop Historical notes","categories":[{"name":"Study","slug":"Study","permalink":"https://y0ngh00n.github.io/categories/Study/"},{"name":"DeepLearning","slug":"Study/DeepLearning","permalink":"https://y0ngh00n.github.io/categories/Study/DeepLearning/"}],"tags":[{"name":"DLB","slug":"DLB","permalink":"https://y0ngh00n.github.io/tags/DLB/"},{"name":"feedforward","slug":"feedforward","permalink":"https://y0ngh00n.github.io/tags/feedforward/"}]},{"title":"Ubuntu 원격 서버 연동하기","slug":"dl-server","date":"2018-03-26T13:45:20.000Z","updated":"2018-08-20T01:42:56.596Z","comments":true,"path":"2018/03/26/dl-server/","link":"","permalink":"https://y0ngh00n.github.io/2018/03/26/dl-server/","excerpt":"","text":"ssh를 이용한 GPU 원격 서버 연동Multi-hop SSH서버 이용을 위해서는 외부서버 접속 -&gt; 내부서버 접속이 필요한 상황이다. 첫번째 터미널 $ ssh -l username -L 6000:inner_ip:22 outer_ip cat - //6000:포트 번호 Pycharm remote interpreter 연동첫번째 터미널을 유지한 상태에서… 프로젝트 remote interpreter 정의참고: https://stackoverflow.com/questions/34359415/pycharm-ssh-interpter-no-such-file-or-directory Here server C is the target machine and server B the gate machine. And then in the remote interpreter configuration. (File&gt;Setting&gt;Projet&gt;project interpreter&gt;click on small wheel button&gt;add remote): host: localhostport: 6000user: user_serverC (for me same as user_serverB) &lt;주의&gt; path mappings를 설정해준다.local: 내 프로젝트 디렉토리remote: 원격 서버에 배포할 디렉토리 deployment(배포) 설정root 디렉토리가 mappings의 위치가 같도록 설정해준다. deploy &amp; run coderemote에 local의 파일이 있어야 run이 된다. jupyter notebook 연동참고: https://glauffer.github.io/2017-04-01-remote-host-a-jupyter-notebook-via-multihop-ssh/첫번째 터미널을 유지한 상태에서… 두번째 터미널… $ ssh -l username -L 8888:localhost:8888 -p 6000 localhost //8888:jupyter port, localhost:host이름 $ ### password login ### $ jupyter notebook --no-browser --port=8888 TODO: pycharm에서 jupyter notebook의 실행이 안되는 것","categories":[{"name":"Study","slug":"Study","permalink":"https://y0ngh00n.github.io/categories/Study/"},{"name":"DeepLearning","slug":"Study/DeepLearning","permalink":"https://y0ngh00n.github.io/categories/Study/DeepLearning/"}],"tags":[{"name":"server","slug":"server","permalink":"https://y0ngh00n.github.io/tags/server/"},{"name":"DL","slug":"DL","permalink":"https://y0ngh00n.github.io/tags/DL/"},{"name":"gpu","slug":"gpu","permalink":"https://y0ngh00n.github.io/tags/gpu/"},{"name":"linux","slug":"linux","permalink":"https://y0ngh00n.github.io/tags/linux/"}]},{"title":"linux 하드웨어 정보 명령어","slug":"linux","date":"2018-03-25T07:01:40.000Z","updated":"2018-03-26T14:32:40.811Z","comments":true,"path":"2018/03/25/linux/","link":"","permalink":"https://y0ngh00n.github.io/2018/03/25/linux/","excerpt":"","text":"리눅스 하드웨어 정보 확인CPU 정보 확인$ cat /proc/cpuinfo | grep \"model name\" | head -1 메모리 정보 확인$ cat /proc/meminfo 디스크 용량 확인$ df -h PCI 정보 확인$ lspci /* 모든 PCI 정보 출력 */ $ lspci | grep RAID /* RAID 모델명 확인 */ $ lspci | grep Ethernet /* 랜카드 모델명 확인 */ $ lspci | grep VGA /* 그래픽 모델명 확인 */","categories":[{"name":"Study","slug":"Study","permalink":"https://y0ngh00n.github.io/categories/Study/"},{"name":"OS","slug":"Study/OS","permalink":"https://y0ngh00n.github.io/categories/Study/OS/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://y0ngh00n.github.io/tags/linux/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-03-22T03:00:46.839Z","updated":"2018-03-26T13:39:30.225Z","comments":true,"path":"2018/03/22/hello-world/","link":"","permalink":"https://y0ngh00n.github.io/2018/03/22/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"Blog","slug":"Blog","permalink":"https://y0ngh00n.github.io/categories/Blog/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://y0ngh00n.github.io/tags/Hexo/"}]}]}